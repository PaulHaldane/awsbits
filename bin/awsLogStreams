#!/usr/bin/env python3

# Based on Diego Link's (https://stackoverflow.com/users/1334028/diego-link) answer at
# https://stackoverflow.com/questions/43327714/my-aws-cloudwatch-bill-is-huge-how-do-i-work-out-which-log-stream-is-causing-it

# Outputs all loggroups with <n> GB of incomingBytes in the past <d> days

previous_days = 30
too_big = 0.01

import boto3
from datetime import datetime as dt
from datetime import timedelta

logs_client = boto3.client('logs')
cloudwatch_client = boto3.client('cloudwatch')

end_date = dt.today().isoformat(timespec='seconds')
start_date = (dt.today() - timedelta(days=previous_days)).isoformat(timespec='seconds')
print("looking from %s to %s" % (start_date, end_date))

paginator = logs_client.get_paginator('describe_log_groups')
pages = paginator.paginate()
for page in pages:
     for json_data in page['logGroups']:
        log_group_name = json_data.get("logGroupName") 

        cw_response = cloudwatch_client.get_metric_statistics(
           Namespace='AWS/Logs',    
           MetricName='IncomingBytes',
           Dimensions=[
            {
                'Name': 'LogGroupName',
                'Value': log_group_name
            },
            ],
            StartTime= start_date,
            EndTime=end_date,
            Period=3600 * 24 * 7,
            Statistics=[
                'Sum'
            ],
            Unit='Bytes'
        )
        if len(cw_response.get("Datapoints")):
            stats_data = cw_response.get("Datapoints")[0]
            stats_sum = stats_data.get("Sum")   
            sum_GB = stats_sum /  (1000 * 1000 * 1000)
            if sum_GB > too_big:
                print("%s = %.3f GB" % (log_group_name , sum_GB))
